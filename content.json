{"meta":{"title":"yeqian data  博客","subtitle":"","description":"hadoop spark hive ","author":"yq","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2020-01-05T08:49:01.000Z","updated":"2020-01-05T08:49:01.464Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"about","date":"2020-01-05T09:06:58.000Z","updated":"2020-01-05T09:06:58.296Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"about","date":"2020-01-05T09:08:35.000Z","updated":"2020-01-05T09:08:35.224Z","comments":true,"path":"about/index-2.html","permalink":"http://yoursite.com/about/index-2.html","excerpt":"","text":""}],"posts":[{"title":"ScalikeJDBC 公共化使用","slug":"scalikejdbc_01","date":"2019-01-23T05:23:34.000Z","updated":"2020-07-26T07:36:13.931Z","comments":true,"path":"2019/01/23/scalikejdbc_01/","link":"","permalink":"http://yoursite.com/2019/01/23/scalikejdbc_01/","excerpt":"","text":"about scalikeJdbcScalikeJDBC是为Scala开发人员提供的一个整洁的基于sql的DB访问库，关于scalikejdbc的使用不在做过多的说明，在下面放出scalikeJdbc 官网。本文着重介绍如何将scalikejdbc 的API进行公共化，做到和其他java持久化组件一样，每次开发持久层只要写少量的代码，减少冗余。http://scalikejdbc.org/ scalikeJdbc 批量插入源码与公共化1、先让我们看一下如何使用batch接口： 123456789101112import scalikejdbc._DB localTx &#123; implicit session =&gt; val batchParams: Seq[Seq[Any]] = (2001 to 3000).map(i =&gt; Seq(i, \"name\" + i)) sql\"insert into emp (id, name) values (?, ?)\".batch(batchParams: _*).apply()&#125;DB localTx &#123; implicit session =&gt; sql\"insert into emp (id, name) values (&#123;id&#125;, &#123;name&#125;)\" .batchByName(Seq(Seq('id -&gt; 1, 'name -&gt; \"Alice\"), Seq('id -&gt; 2, 'name -&gt; \"Bob\")):_*) .apply()&#125; 可以看到如果想批量插入数据，需要定义一个Seq[Seq[Any]]类型的变量，在batch方法中会将该变量打平当成可变参数传入。batch方法同时需要在sql”” 后面才能调用。如果想要通过参数名精确定位某个入参需要传入以’var_name -&gt; value的方式，我们需要搞清楚这些传入值是什么类型如何把他们以公共的方式提取出来，方便之后调用。 2、接下来我们来看一下源码，看一下底层是如何进行调用的：首先我们点进batch方法找到sql”” 的类型 scalikejdbc/SQL.scala 1234 def batch(parameters: Seq[Any]*): SQLBatch = &#123; new SQLBatch(statement, parameters, tags)&#125; 这里new了一个SQLBatch对象，并将已经初始化好的statement和可变参传入。找到方法所在类，看到是一个抽象类, 类名为SQL 123456789101112abstract class SQL[A, E &lt;: WithExtractor]( val statement: String, private[scalikejdbc] val rawParameters: Seq[Any])(f: WrappedResultSet =&gt; A) extends Extractor[A] &#123; ....... def batch(parameters: Seq[Any]*): SQLBatch = &#123; new SQLBatch(statement, parameters, tags) &#125; &#125; 再点进 sql”” 进入SQLInterpolationString类，可以看到返回的正是实例化的SQL类，并且还调用了SQL类的bind方法 12345678910111213class SQLInterpolationString(val s: StringContext) extends AnyVal &#123; import scalikejdbc.interpolation.SQLSyntax def sql[A](params: Any*): SQL[A, NoExtractor] = &#123; val syntax = sqls(params: _*) SQL[A](syntax.value).bind(syntax.rawParameters: _*) &#125; ..........` 这时我们可以确定 sql”” 是一个SQL类的实例。这时我们就可以定义我们的第一个公共方法，批量插入。 123456def batchInsert( sql:SQL[Nothing, NoExtractor],params:Seq[Seq[Any]] ): Unit =&#123; DB localTx &#123; implicit session =&gt; sql.batch(params: _*).apply() &#125;&#125; 我们将可变参和SQL实例提取出来，做为变量传入。调用如下： 12345 def bacthInsert(list: ListBuffer[Seq[Any]]): Unit = &#123; val sql= sql\"REPLACE into dept (dept_name , num ) VALUES(?,?)\"; db.batchInsert(sql,list.toSeq)&#125; 这样我们就可以很方便的调用了，每次只需要写与业务逻辑相关的sql即可 3、我们继续把按名称变量插入方法进行公共化，需要知道 ‘var_name 的类型，点进batchName查看代码： scalikejdbc/SQL.scala 1234567891011121314/** * Binds parameters for batch * * @param parameters parameters * @return SQL for batch */ def batchByName(parameters: Seq[(Symbol, Any)]*): SQLBatch = &#123; val _sql = validateAndConvertToNormalStatement(statement, _settings, parameters.headOption.getOrElse(Seq.empty))._1 val _parameters: Seq[Seq[Any]] = parameters.map &#123; p =&gt; validateAndConvertToNormalStatement(statement, _settings, p)._2 &#125; new SQLBatch(_sql, _parameters, tags) &#125; 看到接收参数为一个可变参数类型为Seq[(Symbol, Any)] 一个Seq里包含了一个Tuple。同样是new了一个SQLBatch类的实例。我们根据之前的batch方法很快就又得到一个公共方法： 12345 def batchByNameInsert( sql:SQL[Nothing, NoExtractor],params:Seq[Seq[(Symbol, Any)]] ): Unit =&#123; DB localTx &#123; implicit session =&gt; sql.batchByName(params: _*).apply() &#125;&#125; 调用如下： 12345def bacthInsertByName(list: ListBuffer[Seq[(Symbol, Any)]]): Unit = &#123; val sqlStr=sql\"REPLACE INTO dept(dept_name,num) values(&#123;dept_name&#125;,&#123;num&#125;)\" db.batchByNameInsert(sqlStr,list.toSeq) &#125; scalikeJdbc 查询接口源码与公共化同样我们先看一下查询api的使用 123val name: List[String] = DB readOnly &#123; implicit session =&gt; sql\"select name from emp\".map(rs =&gt; rs.string(\"name\")).list.apply()&#125; 查询提供map方法可以将结果取出并转成所需要的对象以list输出。 我们来看一下map方法的接受的类型是什么： scalikejdbc/SQL.scala 12345678910/** * Maps values from each [[scalikejdbc.WrappedResultSet]] object. * * @param f extractor function * @tparam A return type * @return SQL instance */def map[A](f: WrappedResultSet =&gt; A): SQL[A, HasExtractor] = &#123; withExtractor[A](f).fetchSize(fetchSize).tags(tags: _*).queryTimeout(queryTimeout)&#125; 接收一个匿名函数作为入参 （f: WrappedResultSet =&gt; A），同样返回的是一个SQL类的实例，这样我们就知道该如何定义我们需要的方法了。 12345def queryAll[A](sql:SQL[Nothing, NoExtractor],f: WrappedResultSet =&gt; A) =&#123; DB readOnly &#123; implicit session=&gt; sql.map(f).list().apply() &#125; &#125; 调用的时候，我们只需要根据每个sql的结果自定义匿名函数即可： 1234567891011def queryAll(): List[RiskWord] =&#123; val sql = sql\"select id,dt,word,count_num from risk_word\" db.queryAll(sql, rs=&gt;&#123; RiskWord(rs.int(\"id\"), rs.date(\"dt\"), rs.string(\"word\"), rs.long(\"count_num\")) &#125; ) &#125; 结语本文对scalike JDBC的几个Operation进行了公共化，scalike还提供了许多方法可以依照上文方法进行公共化。本文也对scalike JDBC的源码做了初步了解知道了每次调用的SQL对象是如何初始化的，接下来需要对scalike源码中参数如何进行绑定sql以及每次调用的详细流程进行了解。 最后附上以上代码的git地址：https://github.com/duskGeek/geek/blob/master/geek-utils/src/main/scala/com/geek/utils/mysql/DBOperations.scala scala ScalikeJdbc","categories":[],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"ScalikeJDBC","slug":"ScalikeJDBC","permalink":"http://yoursite.com/tags/ScalikeJDBC/"}]},{"title":"Spark","slug":"spark","date":"2017-05-13T11:00:00.000Z","updated":"2020-01-05T10:54:15.304Z","comments":true,"path":"2017/05/13/spark/","link":"","permalink":"http://yoursite.com/2017/05/13/spark/","excerpt":"","text":"spark webspark官网 spark demo codeFirst, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). StreamingContext is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second. 123456789import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3// Create a local StreamingContext with two working thread and batch interval of 1 second.// The master requires 2 cores to prevent a starvation scenario.val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")val ssc = new StreamingContext(conf, Seconds(1)) Spark img spark-core spark streaming spark sql","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]}]}