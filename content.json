{"meta":{"title":"yeqian data  博客","subtitle":"","description":"hadoop spark hive ","author":"yq","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2020-01-05T09:06:58.000Z","updated":"2020-01-05T09:06:58.296Z","comments":true,"path":"about/index-1.html","permalink":"http://yoursite.com/about/index-1.html","excerpt":"","text":""},{"title":"about","date":"2020-01-05T09:08:35.000Z","updated":"2020-01-05T09:08:35.224Z","comments":true,"path":"about/index-2.html","permalink":"http://yoursite.com/about/index-2.html","excerpt":"","text":""},{"title":"about","date":"2020-01-05T08:49:01.000Z","updated":"2020-01-05T08:49:01.464Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""}],"posts":[{"title":"Spark","slug":"spark","date":"2017-05-13T11:00:00.000Z","updated":"2020-01-05T10:54:15.304Z","comments":true,"path":"2017/05/13/spark/","link":"","permalink":"http://yoursite.com/2017/05/13/spark/","excerpt":"","text":"spark webspark官网 spark demo codeFirst, we import the names of the Spark Streaming classes and some implicit conversions from StreamingContext into our environment in order to add useful methods to other classes we need (like DStream). StreamingContext is the main entry point for all streaming functionality. We create a local StreamingContext with two execution threads, and a batch interval of 1 second. 123456789import org.apache.spark._import org.apache.spark.streaming._import org.apache.spark.streaming.StreamingContext._ // not necessary since Spark 1.3// Create a local StreamingContext with two working thread and batch interval of 1 second.// The master requires 2 cores to prevent a starvation scenario.val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"NetworkWordCount\")val ssc = new StreamingContext(conf, Seconds(1)) Spark img spark-core spark streaming spark sql","categories":[],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]}]}